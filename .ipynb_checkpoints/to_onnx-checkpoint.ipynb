{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from models.model import ViTPose\n",
    "from configs.ViTPose_huge_coco_256x192 import model as model_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Saved at: /Users/adryw/Documents/ViTPose_pytorch/ckpts/vitpose-25-h_onnx/vitpose-25-h.onnx\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = \"ckpts/vitpose-25-h.pth\"\n",
    "C, H, W = (3, 256, 192)\n",
    "\n",
    "model = ViTPose(model_cfg)\n",
    "ckpt = torch.load(CKPT_PATH, map_location='cpu')\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "\n",
    "output_onnx = 'ckpts/vitpose-25-h_onnx/vitpose-25-h.onnx'\n",
    "input_names = [\"input_0\"]\n",
    "output_names = [\"output_0\"]\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "inputs = torch.randn(1, C, H, W).to(device)\n",
    "\n",
    "dynamic_axes = {'input_0' : {0 : 'batch_size'},\n",
    "                'output_0' : {0 : 'batch_size'}}\n",
    "\n",
    "torch_out = torch.onnx.export(model, inputs, output_onnx, export_params=True, verbose=False,\n",
    "                              input_names=input_names, output_names=output_names, \n",
    "                              opset_version=11, dynamic_axes = dynamic_axes)\n",
    "print(f\">>> Saved at: {os.path.abspath(output_onnx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Original image size: 798 X 720 (height X width)\n",
      ">>> Resized image size: 256 X 192 (height X width)\n",
      ">>> Scale change: 3.1171875, 3.75\n",
      ">>> Output size: (1, 25, 64, 48) ---> 0.4470 sec. elapsed [ 2.2 fps]\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'line'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Visualization \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(points):\n\u001b[0;32m---> 52\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_points_and_skeleton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoints_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoco\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskeleton\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperson_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpoints_color_palette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgist_rainbow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskeleton_color_palette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpoints_palette_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     57\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "File \u001b[0;32m~/Documents/ViTPose_pytorch/utils/visualization.py:194\u001b[0m, in \u001b[0;36mdraw_points_and_skeleton\u001b[0;34m(image, points, skeleton, points_color_palette, points_palette_samples, skeleton_color_palette, skeleton_palette_samples, person_index, confidence_threshold)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_points_and_skeleton\u001b[39m(image, points, skeleton, points_color_palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtab20\u001b[39m\u001b[38;5;124m'\u001b[39m, points_palette_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m    164\u001b[0m                              skeleton_color_palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSet2\u001b[39m\u001b[38;5;124m'\u001b[39m, skeleton_palette_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, person_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    165\u001b[0m                              confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    Draws `points` and `skeleton` on `image`.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_skeleton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskeleton\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_palette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskeleton_color_palette\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mpalette_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskeleton_palette_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperson_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperson_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     image \u001b[38;5;241m=\u001b[39m draw_points(image, points, color_palette\u001b[38;5;241m=\u001b[39mpoints_color_palette, palette_samples\u001b[38;5;241m=\u001b[39mpoints_palette_samples,\n\u001b[1;32m    198\u001b[0m                         confidence_threshold\u001b[38;5;241m=\u001b[39mconfidence_threshold)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/Documents/ViTPose_pytorch/utils/visualization.py:155\u001b[0m, in \u001b[0;36mdraw_skeleton\u001b[0;34m(image, points, skeleton, color_palette, palette_samples, person_index, confidence_threshold)\u001b[0m\n\u001b[1;32m    153\u001b[0m     pt1, pt2 \u001b[38;5;241m=\u001b[39m points[joint]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pt1[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m confidence_threshold \u001b[38;5;129;01mand\u001b[39;00m pt2[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m confidence_threshold:\n\u001b[0;32m--> 155\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpt1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpt1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpt2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpt2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperson_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'line'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = \"examples/img1.jpg\"\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from time import time\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from utils.visualization import draw_points_and_skeleton, joints_dict\n",
    "from utils.dist_util import get_dist_info, init_dist\n",
    "from utils.top_down_eval import keypoints_from_heatmaps\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(output_onnx)\n",
    "\n",
    "# Prepare input data\n",
    "img = Image.open(IMG_PATH)\n",
    "\n",
    "org_w, org_h = img.size\n",
    "print(f\">>> Original image size: {org_h} X {org_w} (height X width)\")\n",
    "print(f\">>> Resized image size: {H} X {W} (height X width)\")\n",
    "print(f\">>> Scale change: {org_h/H}, {org_w/W}\")\n",
    "img_tensor = transforms.Compose (\n",
    "    [transforms.Resize((H, W)),\n",
    "        transforms.ToTensor()]\n",
    ")(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# Feed to model\n",
    "tic = time()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_tensor)}\n",
    "heatmaps = ort_session.run(None, ort_inputs)[0]\n",
    "# heatmaps = vit_pose(img_tensor).detach().cpu().numpy() # N, 17, h/4, w/4\n",
    "elapsed_time = time()-tic\n",
    "print(f\">>> Output size: {heatmaps.shape} ---> {elapsed_time:.4f} sec. elapsed [{elapsed_time**-1: .1f} fps]\\n\")    \n",
    "\n",
    "# points = heatmap2coords(heatmaps=heatmaps, original_resolution=(org_h, org_w))\n",
    "points, prob = keypoints_from_heatmaps(heatmaps=heatmaps, center=np.array([[org_w//2, org_h//2]]), scale=np.array([[org_w, org_h]]),\n",
    "                                        unbiased=True, use_udp=True)\n",
    "points = np.concatenate([points[:, :, ::-1], prob], axis=2)\n",
    "\n",
    "# Visualization \n",
    "for pid, point in enumerate(points):\n",
    "    img = draw_points_and_skeleton(img.copy(), point, joints_dict()['coco']['skeleton'], person_index=pid,\n",
    "                                    points_color_palette='gist_rainbow', skeleton_color_palette='jet',\n",
    "                                    points_palette_samples=10, confidence_threshold=0.4)\n",
    "    \n",
    "    plt.figure(figsize=(5,10))\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Result\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
